{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86cfda83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2218574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac4132d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f53cba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Lung_Cancer_Diagnostic\n"
     ]
    }
   ],
   "source": [
    "cd D:\\Lung_Cancer_Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffb785a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Lung_Cancer_Diagnostic\\LunaDataset\n"
     ]
    }
   ],
   "source": [
    "cd D:\\Lung_Cancer_Diagnostic\\LunaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21327da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from util.util import enumerateWithEstimate\n",
    "from dsets import LunaDataset\n",
    "from util.logconf import logging\n",
    "from model import LunaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc6a75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.WARN)\n",
    "log.setLevel(logging.INFO)\n",
    "log.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62e7c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_LABEL_NDX=0\n",
    "METRICS_PRED_NDX=1\n",
    "METRICS_LOSS_NDX=2\n",
    "METRICS_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaTrainingApp:\n",
    "    def __int__(self, sys_argv=None):\n",
    "        if sys_argv is None:\n",
    "            sys_argv = sys.argv[1:]\n",
    "        \n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--num-workers',\n",
    "                           help='Number of worker processes for background data loding',\n",
    "                           default=8,\n",
    "                           type=int,\n",
    "                           )\n",
    "        parser.add_argument('--batch-size',\n",
    "                            help='Batch size to use for training',\n",
    "                            default=32,\n",
    "                            type=int,\n",
    "                           )\n",
    "        parser.add_argument('--epochs',\n",
    "                           help='Number of epochs to train for',\n",
    "                           default=1,\n",
    "                           type=int,\n",
    "                           )\n",
    "        parser.add_argument('--tb-prefix',\n",
    "                           default='p2ch11',\n",
    "                           help=\"Data prefix to use for Tensorboard run. Defaults to chapter.\",\n",
    "                           )\n",
    "        parser.add_argument('comment',\n",
    "                           help='Comment suffix for Tensorboard run.',\n",
    "                           nargs='?',\n",
    "                           default='dwlpt',\n",
    "                           )\n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "        self.time_str = datetime.datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\")\n",
    "        \n",
    "        self.trn_writer = None\n",
    "        self.val_writer = None\n",
    "        self.totalTrainingSamples_count = 0\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "        \n",
    "        self.model = self.initModel()\n",
    "        self.optimizer = self.initOptimizer()\n",
    "        \n",
    "    def initModel(self):\n",
    "        model = LunaModel()\n",
    "        if self.use_cuda:\n",
    "            log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                model = nn.DataParallel(model)\n",
    "            model = model.to(self.device)\n",
    "        return model\n",
    "    \n",
    "    def initOptimizer(self):\n",
    "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
    "    \n",
    "    def initTrainDl(self):\n",
    "        train_ds = LunaDataset(\n",
    "            val_stride = 10,\n",
    "            isValSet_bool = False,\n",
    "        )\n",
    "        \n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "            \n",
    "        train_dl = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=self.use_cuda,\n",
    "        )\n",
    "        \n",
    "        return train_dl\n",
    "    \n",
    "    def initValDl(self):\n",
    "        val_ds = LunaDataset(\n",
    "            val_stride=10,\n",
    "            isValSet_bool=True,\n",
    "        )\n",
    "        \n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "            \n",
    "        val_dl = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size = batch_size,\n",
    "            num_workers = self.cli_args.num_workers,\n",
    "            pin_memory = self.use_cuda,\n",
    "        )\n",
    "        \n",
    "        return val_dl\n",
    "    \n",
    "    def initTensorboardWriters(self):\n",
    "        if self.trn_writer is None:\n",
    "            log_dir = os.path.join('runs', self.cli_args.tb_prefix, self.time_str)\n",
    "            \n",
    "            self.trn_writer = SummaryWriter(\n",
    "                log_dir = log_dir + '-trn_cls-' + self.cli_args.comment)\n",
    "            self.val_writer = SummaryWriter(\n",
    "                log_dir = log_dir + '-val_cls-' + self.cli_args.comment)\n",
    "            \n",
    "    def main(self):\n",
    "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "        \n",
    "        train_dl = self.initTrainDl()\n",
    "        val_dl = self.initValDl()\n",
    "        \n",
    "        for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
    "            \n",
    "            log.info('Epoch {} of {}, {}/{} batches of size {}*{}'.format(\n",
    "                epoch_ndx,\n",
    "                self.cli_args.epochs,\n",
    "                len(train_dl),\n",
    "                len(val_dl),\n",
    "                self.cli_args.batch_size,\n",
    "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
    "            ))\n",
    "            \n",
    "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
    "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "            \n",
    "            valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
    "            self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "            \n",
    "        if hasattr(self, 'trn_writer'):\n",
    "            self.trn_writer.close()\n",
    "            self.val_writer.close()\n",
    "            \n",
    "    def doTraining(self, epoch_ndx, train_dl):\n",
    "        self.model.train()\n",
    "        trnMetrics_g = torch.zeros(\n",
    "            METRICS_SIZE,\n",
    "            len(train_dl.dataset),\n",
    "            device=self.device,\n",
    "        )\n",
    "        \n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            train_dl,\n",
    "            \"E{} Training\".format(epoch_ndx),\n",
    "            start_ndx = train_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, batch_tup in batch_iter:\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            loss_var = self.computeBatchLoss(\n",
    "                batch_ndx,\n",
    "                batch_tup,\n",
    "                train_dl.batch_size,\n",
    "                trnMetrics_g\n",
    "            )\n",
    "            \n",
    "            loss_var.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # This is for adding the model graph to TensorBoard.\n",
    "            # if epoch_ndx == 1 and batch_ndx == 0:\n",
    "            #      with torch.no_grad():\n",
    "            #            model = LunaModel()\n",
    "            #            self.trn_writer.add_graph(model, batch_tup[0], verbose=True)\n",
    "            #            self.trn_writer.close()\n",
    "            \n",
    "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
    "        \n",
    "        return trnMetrics_g.to('cpu')\n",
    "    \n",
    "    def doValidation(self, epoch_ndx, val_dl):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            valMetrics_g = torch.zeros(\n",
    "                METRICS_SIZE,\n",
    "                len(val_dl.dataset),\n",
    "                device=self.device,\n",
    "            )\n",
    "            \n",
    "            batch_iter = enumerateWithEstimate(\n",
    "                    val_dl,\n",
    "                    \"E{} Validation \".format(epoch_ndx),\n",
    "                    start_ndx=val_dl.num_workers,\n",
    "            )\n",
    "            for batch_ndx, batch_tup in batch_iter:\n",
    "                self.computeBatchLoss(\n",
    "                    batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
    "                \n",
    "        return valMetrics_g.to(\"cpu\")\n",
    "\n",
    "    \n",
    "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
    "        input_t, label_t, _series_list, _center_list = batch_tup\n",
    "        \n",
    "        input_g = input_t.to(self.device, non_blocking=True)\n",
    "        label_g = label_t.to(self.device, non_blocking=True)\n",
    "        \n",
    "        logits_g, probability_g = self.model(input_g)\n",
    "        \n",
    "        loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "        loss_g = loss_func(\n",
    "            logits_g,\n",
    "            label_g[:,1],\n",
    "        )\n",
    "        start_ndx = batch_ndx * batch_size\n",
    "        end_ndx = start_ndx + label_t.size(0)\n",
    "        \n",
    "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = \\\n",
    "            label_g[:,1].detach()\n",
    "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = \\\n",
    "            probability_g[:,1].detach()\n",
    "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = \\\n",
    "            loss_g.detach()\n",
    "        \n",
    "        return loss_g.mean()\n",
    "    \n",
    "    def logMetrics(\n",
    "        ## 작성하기\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
